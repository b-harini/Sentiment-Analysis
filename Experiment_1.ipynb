{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import re\n",
        "import os\n",
        "\n",
        "# Function to fetch archive links based on the provided URL structure\n",
        "def fetch_archive_links(base_url, start_year=2015, end_year=2024):\n",
        "    links = []\n",
        "    months = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\n",
        "\n",
        "    for year in range(start_year, end_year + 1):\n",
        "        for month in months:\n",
        "            month_name = f\"{year}-{month}\"\n",
        "            link = f\"{base_url}{month_name}.txt\"\n",
        "            links.append(link)\n",
        "\n",
        "    return links\n",
        "\n",
        "# Function to download the file and return its content\n",
        "def download_file(url):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        return response.text\n",
        "    return None\n",
        "\n",
        "# Main processing function\n",
        "def fetch_and_save_mailing_list(base_url, output_file, start_year=2015, end_year=2024):\n",
        "    archive_links = fetch_archive_links(base_url, start_year, end_year)\n",
        "    all_emails = []\n",
        "\n",
        "    for link in archive_links:\n",
        "        print(f\"Processing {link}\")\n",
        "        content = download_file(link)\n",
        "        if content:\n",
        "            # Split by email messages (simple regex)\n",
        "            emails = re.split(r'\\nFrom ', content)\n",
        "            all_emails.extend(emails)\n",
        "\n",
        "    # Save the raw email content to a text file\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        for email in all_emails:\n",
        "            f.write(\"========== EMAIL ==========\\n\")\n",
        "            f.write(email)\n",
        "    print(f\"Data saved to {output_file}\")\n",
        "\n",
        "# Define base URL and output file path\n",
        "base_url = 'https://mail.python.org/pipermail/db-sig/'\n",
        "raw_output_file = '/content/db_emails_raw.txt'\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(os.path.dirname(raw_output_file), exist_ok=True)\n",
        "\n",
        "# Fetch and save the mailing list data\n",
        "fetch_and_save_mailing_list(base_url, raw_output_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MAr7lw3OKZS",
        "outputId": "f4c0318f-3e84-4ad7-cf95-65c91500c810"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing https://mail.python.org/pipermail/db-sig/2015-January.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2015-February.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2015-March.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2015-April.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2015-May.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2015-June.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2015-July.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2015-August.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2015-September.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2015-October.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2015-November.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2015-December.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2016-January.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2016-February.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2016-March.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2016-April.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2016-May.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2016-June.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2016-July.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2016-August.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2016-September.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2016-October.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2016-November.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2016-December.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2017-January.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2017-February.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2017-March.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2017-April.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2017-May.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2017-June.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2017-July.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2017-August.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2017-September.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2017-October.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2017-November.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2017-December.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2018-January.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2018-February.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2018-March.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2018-April.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2018-May.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2018-June.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2018-July.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2018-August.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2018-September.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2018-October.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2018-November.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2018-December.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2019-January.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2019-February.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2019-March.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2019-April.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2019-May.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2019-June.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2019-July.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2019-August.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2019-September.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2019-October.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2019-November.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2019-December.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2020-January.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2020-February.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2020-March.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2020-April.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2020-May.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2020-June.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2020-July.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2020-August.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2020-September.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2020-October.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2020-November.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2020-December.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2021-January.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2021-February.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2021-March.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2021-April.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2021-May.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2021-June.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2021-July.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2021-August.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2021-September.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2021-October.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2021-November.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2021-December.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2022-January.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2022-February.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2022-March.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2022-April.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2022-May.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2022-June.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2022-July.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2022-August.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2022-September.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2022-October.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2022-November.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2022-December.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2023-January.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2023-February.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2023-March.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2023-April.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2023-May.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2023-June.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2023-July.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2023-August.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2023-September.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2023-October.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2023-November.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2023-December.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2024-January.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2024-February.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2024-March.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2024-April.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2024-May.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2024-June.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2024-July.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2024-August.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2024-September.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2024-October.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2024-November.txt\n",
            "Processing https://mail.python.org/pipermail/db-sig/2024-December.txt\n",
            "Data saved to /content/db_emails_raw.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SENTIMENT ANALYSIS USING TEXTBLOB"
      ],
      "metadata": {
        "id": "JygFcKOZNPZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from textblob import TextBlob\n",
        "import pandas as pd\n",
        "\n",
        "# Load the email data from the newly created file\n",
        "file_path_new_mobile = '/content/db_emails_raw.txt'\n",
        "with open(file_path_new_mobile, 'r') as file:\n",
        "    email_data_new_mobile = file.read()\n",
        "\n",
        "# Split the data into individual emails\n",
        "emails_new_mobile = re.split(r\"========== EMAIL ==========\\n\", email_data_new_mobile)\n",
        "\n",
        "# Define important keywords/topics for analysis in a mobile development context\n",
        "important_keywords_mobile = ['database']\n",
        "\n",
        "# Extract date, topic, and sentiment details from the new dataset\n",
        "def extract_date_topic_sentiment_mobile(emails, keywords):\n",
        "    discussion_details = []\n",
        "\n",
        "    for email in emails:\n",
        "        # Extract the date of the email\n",
        "        date_match = re.search(r'\\nDate: (.+)\\n', email)\n",
        "        if date_match:\n",
        "            email_date = date_match.group(1).strip()\n",
        "            try:\n",
        "                email_date = pd.to_datetime(email_date, utc=True)\n",
        "            except ValueError:\n",
        "                continue  # Skip email if date format is not recognized\n",
        "\n",
        "            # Analyze sentiment of the email\n",
        "            blob = TextBlob(email)\n",
        "            sentiment = blob.sentiment.polarity\n",
        "            sentiment_type = 'neutral'\n",
        "            if sentiment > 0:\n",
        "                sentiment_type = 'positive'\n",
        "            elif sentiment < 0:\n",
        "                sentiment_type = 'negative'\n",
        "\n",
        "            # Check for mentions of important topics\n",
        "            for keyword in keywords:\n",
        "                if keyword in email.lower():\n",
        "                    discussion_details.append({\n",
        "                        'Date': email_date,\n",
        "                        'Topic': keyword,\n",
        "                        'Sentiment': sentiment_type\n",
        "                    })\n",
        "\n",
        "    return discussion_details\n",
        "\n",
        "# Perform the extraction\n",
        "discussion_details_new_mobile = extract_date_topic_sentiment_mobile(emails_new_mobile, important_keywords_mobile)\n",
        "\n",
        "# Convert to DataFrame for further analysis\n",
        "df_discussion_details_new_mobile = pd.DataFrame(discussion_details_new_mobile)\n",
        "\n",
        "# Group the data by year and topic to analyze the discussion distribution\n",
        "df_discussion_details_new_mobile['Year'] = df_discussion_details_new_mobile['Date'].dt.year\n",
        "discussion_by_year_new_mobile = df_discussion_details_new_mobile.groupby(['Year', 'Topic']).size().unstack(fill_value=0)\n",
        "\n",
        "# Overall sentiment analysis\n",
        "overall_sentiment_mobile = df_discussion_details_new_mobile.groupby('Topic')['Sentiment'].value_counts().unstack(fill_value=0)\n",
        "\n",
        "# Combine the number of mentions and sentiment data for each topic\n",
        "combined_data_mobile = pd.concat([discussion_by_year_new_mobile.sum(axis=0), overall_sentiment_mobile], axis=1).fillna(0)\n",
        "\n",
        "# Adjust the number of column names based on the actual columns in the DataFrame\n",
        "combined_data_mobile.columns = ['Total Mentions'] + list(overall_sentiment_mobile.columns)\n",
        "\n",
        "# Save the results to a text file\n",
        "output_file = '/content/db_discussion_details.txt'\n",
        "with open(output_file, 'w') as f:\n",
        "    f.write(\"Combined Topic Analysis for DB Mailing List\\n\")\n",
        "    f.write(\"===============================================\\n\\n\")\n",
        "\n",
        "    f.write(\"Overall Combined Data:\\n\")\n",
        "    for topic, data in combined_data_mobile.iterrows():\n",
        "        f.write(f\"Topic: {topic}\\n\")\n",
        "        f.write(f\"  Total Mentions: {int(data['Total Mentions'])}\\n\")\n",
        "        for sentiment in overall_sentiment_mobile.columns:\n",
        "            f.write(f\"  {sentiment.capitalize()} Sentiment: {int(data[sentiment])}\\n\")\n",
        "        f.write(\"\\n\")\n",
        "print(f\"Results saved to {output_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYapSsH6h9x-",
        "outputId": "6aeb0b23-1cff-44a1-dddb-f1beafc5e266"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to /content/db_discussion_details.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DOWNLOAD THE RESULT FILE"
      ],
      "metadata": {
        "id": "msn3p33jNXDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(output_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "HcKi36GyOKfx",
        "outputId": "918dc54e-4874-4bf5-be96-7a1cac39dafd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_828990c2-7ea5-4fce-a0db-c81f3b1866f7\", \"db_discussion_details.txt\", 206)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SENTIMENT ANALYSIS USING VADER"
      ],
      "metadata": {
        "id": "KXEnemRONab0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import pandas as pd\n",
        "import nltk\n",
        "\n",
        "# Download the VADER lexicon\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Load the email data from the newly created file\n",
        "file_path_new_mobile = '/content/db_emails_raw.txt'\n",
        "with open(file_path_new_mobile, 'r') as file:\n",
        "    email_data_new_mobile = file.read()\n",
        "\n",
        "# Split the data into individual emails\n",
        "emails_new_mobile = re.split(r\"========== EMAIL ==========\\n\", email_data_new_mobile)\n",
        "\n",
        "# Define important keywords/topics for analysis in a mobile development context\n",
        "important_keywords_mobile = ['database']\n",
        "\n",
        "\n",
        "# Initialize VADER sentiment intensity analyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Extract date, topic, and sentiment details from the new dataset\n",
        "def extract_date_topic_sentiment_mobile(emails, keywords):\n",
        "    discussion_details = []\n",
        "\n",
        "    for email in emails:\n",
        "        # Extract the date of the email\n",
        "        date_match = re.search(r'\\nDate: (.+)\\n', email)\n",
        "        if date_match:\n",
        "            email_date = date_match.group(1).strip()\n",
        "            try:\n",
        "                email_date = pd.to_datetime(email_date, utc=True)\n",
        "            except ValueError:\n",
        "                continue  # Skip email if date format is not recognized\n",
        "\n",
        "            # Analyze sentiment of the email using VADER\n",
        "            sentiment_score = sia.polarity_scores(email)\n",
        "            sentiment_type = 'neutral'\n",
        "            if sentiment_score['compound'] > 0:\n",
        "                sentiment_type = 'positive'\n",
        "            elif sentiment_score['compound'] < 0:\n",
        "                sentiment_type = 'negative'\n",
        "\n",
        "            # Check for mentions of important topics\n",
        "            for keyword in keywords:\n",
        "                if keyword in email.lower():\n",
        "                    discussion_details.append({\n",
        "                        'Date': email_date,\n",
        "                        'Topic': keyword,\n",
        "                        'Sentiment': sentiment_type\n",
        "                    })\n",
        "\n",
        "    return discussion_details\n",
        "\n",
        "# Perform the extraction\n",
        "discussion_details_new_mobile = extract_date_topic_sentiment_mobile(emails_new_mobile, important_keywords_mobile)\n",
        "\n",
        "# Convert to DataFrame for further analysis\n",
        "df_discussion_details_new_mobile = pd.DataFrame(discussion_details_new_mobile)\n",
        "\n",
        "# Group the data by year and topic to analyze the discussion distribution\n",
        "df_discussion_details_new_mobile['Year'] = df_discussion_details_new_mobile['Date'].dt.year\n",
        "discussion_by_year_new_mobile = df_discussion_details_new_mobile.groupby(['Year', 'Topic']).size().unstack(fill_value=0)\n",
        "\n",
        "# Overall sentiment analysis\n",
        "overall_sentiment_mobile = df_discussion_details_new_mobile.groupby('Topic')['Sentiment'].value_counts().unstack(fill_value=0)\n",
        "\n",
        "# Combine the number of mentions and sentiment data for each topic\n",
        "combined_data_mobile = pd.concat([discussion_by_year_new_mobile.sum(axis=0), overall_sentiment_mobile], axis=1).fillna(0)\n",
        "\n",
        "# Inspect combined_data_mobile to understand its structure\n",
        "print(\"Columns in combined_data_mobile:\")\n",
        "print(combined_data_mobile.columns)\n",
        "\n",
        "# Assign appropriate labels based on the number of columns\n",
        "if len(combined_data_mobile.columns) == 4:\n",
        "    combined_data_mobile.columns = ['Total Mentions', 'Negative Sentiment', 'Neutral Sentiment', 'Positive Sentiment']\n",
        "elif len(combined_data_mobile.columns) == 3:\n",
        "    combined_data_mobile.columns = ['Total Mentions', 'Negative Sentiment', 'Positive Sentiment']\n",
        "\n",
        "# Save the results to a text file\n",
        "output_file = '/content/db_discussion_details_vader.txt'\n",
        "with open(output_file, 'w') as f:\n",
        "    f.write(\"Combined Topic Analysis for DB Development Mailing List\\n\")\n",
        "    f.write(\"===============================================\\n\\n\")\n",
        "\n",
        "    f.write(\"Overall Combined Data:\\n\")\n",
        "    for topic, data in combined_data_mobile.iterrows():\n",
        "        f.write(f\"Topic: {topic}\\n\")\n",
        "        f.write(f\"  Total Mentions: {int(data['Total Mentions'])}\\n\")\n",
        "        f.write(f\"  Negative Sentiment: {int(data['Negative Sentiment'])}\\n\")\n",
        "        if 'Neutral Sentiment' in data:\n",
        "            f.write(f\"  Neutral Sentiment: {int(data['Neutral Sentiment'])}\\n\")\n",
        "        f.write(f\"  Positive Sentiment: {int(data['Positive Sentiment'])}\\n\\n\")\n",
        "print(f\"Results saved to {output_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXZKLCYCOKle",
        "outputId": "b4e82d6b-a255-4462-db08-c7c934a78f42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in combined_data_mobile:\n",
            "Index([0, 'negative', 'positive'], dtype='object')\n",
            "Results saved to /content/db_discussion_details_vader.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DOWNLOAD THE RESULT FILE"
      ],
      "metadata": {
        "id": "jT4z915rNe4G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(output_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "7IMu_n9cOKo3",
        "outputId": "eb641f70-2a91-4cfd-d554-e46f1c94e811"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_44ed4221-1565-4ef0-a58c-c1b667467d69\", \"db_discussion_details_vader.txt\", 217)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SENTIMENT ANALYSIS USING BERT AND DISTILBERT"
      ],
      "metadata": {
        "id": "04LtIKUQNhz9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from transformers import pipeline\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "# Detect if GPU is available\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "\n",
        "# Load the email data from the newly created file\n",
        "file_path_new_mobile = '/content/db_emails_raw.txt'\n",
        "with open(file_path_new_mobile, 'r') as file:\n",
        "    email_data_new_mobile = file.read()\n",
        "\n",
        "# Split the data into individual emails\n",
        "emails_new_mobile = re.split(r\"========== EMAIL ==========\\n\", email_data_new_mobile)\n",
        "\n",
        "# Define important keywords/topics for analysis in a mobile development context\n",
        "important_keywords_mobile = ['database']\n",
        "\n",
        "\n",
        "# Initialize BERT and DistilBERT sentiment analysis pipelines with truncation\n",
        "bert_classifier = pipeline('sentiment-analysis', model='nlptown/bert-base-multilingual-uncased-sentiment', device=device, max_length=512, truncation=True)\n",
        "distilbert_classifier = pipeline('sentiment-analysis', model='distilbert-base-uncased-finetuned-sst-2-english', device=device, max_length=512, truncation=True)\n",
        "\n",
        "# Extract date, topic, and sentiment details from the new dataset using BERT and DistilBERT\n",
        "def extract_date_topic_sentiment_mobile(emails, keywords):\n",
        "    discussion_details = []\n",
        "\n",
        "    for email in emails:\n",
        "        # Extract the date of the email\n",
        "        date_match = re.search(r'\\nDate: (.+)\\n', email)\n",
        "        if date_match:\n",
        "            email_date = date_match.group(1).strip()\n",
        "            try:\n",
        "                email_date = pd.to_datetime(email_date, utc=True)\n",
        "            except ValueError:\n",
        "                continue  # Skip email if date format is not recognized\n",
        "\n",
        "            # Analyze sentiment of the email using BERT and DistilBERT - Truncation is handled within the pipelines\n",
        "            bert_sentiment = bert_classifier(email)[0]['label']\n",
        "            distilbert_sentiment = distilbert_classifier(email)[0]['label']\n",
        "\n",
        "            # Check for mentions of important topics\n",
        "            for keyword in keywords:\n",
        "                if keyword in email.lower():\n",
        "                    discussion_details.append({\n",
        "                        'Date': email_date,\n",
        "                        'Topic': keyword,\n",
        "                        'BERT Sentiment': bert_sentiment,\n",
        "                        'DistilBERT Sentiment': distilbert_sentiment\n",
        "                    })\n",
        "\n",
        "    return discussion_details\n",
        "\n",
        "# Perform the extraction\n",
        "discussion_details_new_mobile = extract_date_topic_sentiment_mobile(emails_new_mobile, important_keywords_mobile)\n",
        "\n",
        "# Convert to DataFrame for further analysis\n",
        "df_discussion_details_new_mobile = pd.DataFrame(discussion_details_new_mobile)\n",
        "\n",
        "# Group the data by year and topic to analyze the discussion distribution\n",
        "df_discussion_details_new_mobile['Year'] = df_discussion_details_new_mobile['Date'].dt.year\n",
        "discussion_by_year_new_mobile = df_discussion_details_new_mobile.groupby(['Year', 'Topic']).size().unstack(fill_value=0)\n",
        "\n",
        "# Overall sentiment analysis using BERT and DistilBERT\n",
        "overall_sentiment_bert = df_discussion_details_new_mobile.groupby('Topic')['BERT Sentiment'].value_counts().unstack(fill_value=0)\n",
        "overall_sentiment_distilbert = df_discussion_details_new_mobile.groupby('Topic')['DistilBERT Sentiment'].value_counts().unstack(fill_value=0)\n",
        "\n",
        "# Combine the number of mentions and sentiment data for each topic\n",
        "combined_data_mobile_bert = pd.concat([discussion_by_year_new_mobile.sum(axis=0), overall_sentiment_bert], axis=1).fillna(0)\n",
        "combined_data_mobile_distilbert = pd.concat([discussion_by_year_new_mobile.sum(axis=0), overall_sentiment_distilbert], axis=1).fillna(0)\n",
        "\n",
        "# Handle the column names dynamically based on the actual data structure\n",
        "columns_bert = ['Total Mentions'] + list(overall_sentiment_bert.columns)\n",
        "columns_distilbert = ['Total Mentions'] + list(overall_sentiment_distilbert.columns)\n",
        "combined_data_mobile_bert.columns = columns_bert\n",
        "combined_data_mobile_distilbert.columns = columns_distilbert\n",
        "\n",
        "# Save the BERT results to a text file\n",
        "output_file_bert = '/content/db_discussion_details_bert.txt'\n",
        "with open(output_file_bert, 'w') as f_bert:\n",
        "    f_bert.write(\"Combined Topic Analysis for DB Mailing List (BERT)\\n\")\n",
        "    f_bert.write(\"===============================================================\\n\\n\")\n",
        "\n",
        "    f_bert.write(\"Overall Combined Data:\\n\")\n",
        "    for topic, data in combined_data_mobile_bert.iterrows():\n",
        "        f_bert.write(f\"Topic: {topic}\\n\")\n",
        "        for column in combined_data_mobile_bert.columns:\n",
        "            f_bert.write(f\"  {column}: {int(data[column])}\\n\")\n",
        "        f_bert.write(\"\\n\")\n",
        "\n",
        "print(f\"BERT results saved to {output_file_bert}\")\n",
        "\n",
        "# Save the DistilBERT results to a text file\n",
        "output_file_distilbert = '/content/db_discussion_details_distilbert.txt'\n",
        "with open(output_file_distilbert, 'w') as f_distilbert:\n",
        "    f_distilbert.write(\"Combined Topic Analysis for DB Mailing List (DistilBERT)\\n\")\n",
        "    f_distilbert.write(\"======================================================================\\n\\n\")\n",
        "\n",
        "    f_distilbert.write(\"Overall Combined Data:\\n\")\n",
        "    for topic, data in combined_data_mobile_distilbert.iterrows():\n",
        "        f_distilbert.write(f\"Topic: {topic}\\n\")\n",
        "        for column in combined_data_mobile_distilbert.columns:\n",
        "            f_distilbert.write(f\"  {column}: {int(data[column])}\\n\")\n",
        "        f_distilbert.write(\"\\n\")\n",
        "\n",
        "print(f\"DistilBERT results saved to {output_file_distilbert}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ztMIBec0Xz0",
        "outputId": "9c1d2c6c-6e7c-4c34-dc6a-ae205df2420d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERT results saved to /content/db_discussion_details_bert.txt\n",
            "DistilBERT results saved to /content/db_discussion_details_distilbert.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DOWNLOAD THE RESULT FILE"
      ],
      "metadata": {
        "id": "EhOg1H5oNpJN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(output_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Tdiul4UPSOpw",
        "outputId": "935554d6-f09c-4f62-d5d1-649538443517"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_d6a84ff7-1f9f-4f80-941f-3c3a221bff40\", \"email_discussion_details_vader.txt\", 897)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}