{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "DATA FETCH AND PRE-PROCESS FOR EMAIL MAILING LIST\n"
      ],
      "metadata": {
        "id": "Qhd6ynN2ORQP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import re\n",
        "import os\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import time\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from transformers import pipeline\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Function to fetch archive links based on the provided URL structure\n",
        "def fetch_archive_links(base_url, start_year=2015, end_year=2024):\n",
        "    links = []\n",
        "    months = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\n",
        "\n",
        "    for year in range(start_year, end_year + 1):\n",
        "        for month in months:\n",
        "            month_name = f\"{year}-{month}\"\n",
        "            link = f\"{base_url}{month_name}.txt\"\n",
        "            links.append(link)\n",
        "\n",
        "    return links\n",
        "\n",
        "# Function to download the file and return its content\n",
        "def download_file(url):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        return response.text\n",
        "    return None\n",
        "\n",
        "# Main processing function\n",
        "def fetch_and_save_mailing_list(base_url, output_file, start_year=2015, end_year=2024):\n",
        "    archive_links = fetch_archive_links(base_url, start_year, end_year)\n",
        "    all_emails = []\n",
        "\n",
        "    for link in archive_links:\n",
        "        print(f\"Processing {link}\")\n",
        "        content = download_file(link)\n",
        "        if content:\n",
        "            # Split by email messages (simple regex)\n",
        "            emails = re.split(r'\\nFrom ', content)\n",
        "            all_emails.extend(emails)\n",
        "\n",
        "    # Ensure the output directory exists\n",
        "    output_directory = os.path.dirname(output_file)\n",
        "    os.makedirs(output_directory, exist_ok=True)\n",
        "\n",
        "    # Save the raw data to a text file for preprocessing\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        for email in all_emails:\n",
        "            f.write(email + '\\n')\n",
        "    print(f\"Data saved to {output_file}\")\n",
        "\n",
        "# Function to clean and normalize text\n",
        "def clean_text_chunk(text_chunk):\n",
        "    text_chunk = re.sub(r'[\\r\\n]+', ' ', text_chunk)  # Remove newlines and carriage returns\n",
        "    text_chunk = re.sub(r'\\s+', ' ', text_chunk)  # Remove multiple spaces\n",
        "    text_chunk = re.sub(r'From:.+?Subject:.+?Date:.+?\\d{4}', ' ', text_chunk)  # Remove email headers and footers\n",
        "    return text_chunk\n",
        "\n",
        "def tokenize_and_normalize_chunk(text_chunk, stop_words):\n",
        "    text_chunk = text_chunk.lower()  # Convert text to lowercase\n",
        "    words = nltk.word_tokenize(text_chunk)  # Tokenize the text\n",
        "    words = [word for word in words if word.isalnum() and word not in stop_words]  # Remove stop words and non-alphanumeric tokens\n",
        "    return words\n",
        "\n",
        "# Function to preprocess the text file in chunks\n",
        "def preprocess_text_file_in_chunks(input_file_path, output_file_path, chunk_size=1024*1024):\n",
        "    start_time = time.time()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    with open(input_file_path, 'r', encoding='utf-8') as infile, open(output_file_path, 'w', encoding='utf-8') as outfile:\n",
        "        while True:\n",
        "            text_chunk = infile.read(chunk_size)\n",
        "            if not text_chunk:\n",
        "                break\n",
        "            # Split the chunk into documents/emails\n",
        "            documents = text_chunk.split('\\nFrom ')\n",
        "            for document in documents:\n",
        "                if document.strip():\n",
        "                    cleaned_chunk = clean_text_chunk(document)  # Clean the text chunk\n",
        "                    processed_words = tokenize_and_normalize_chunk(cleaned_chunk, stop_words)  # Tokenize and normalize the text chunk\n",
        "                    processed_text = ' '.join(processed_words)  # Join the words back into a single string\n",
        "                    outfile.write(processed_text + '\\n')  # Write each processed document as a separate line\n",
        "\n",
        "    print(f\"Preprocessed text saved to '{output_file_path}'.\")\n",
        "    print(f\"Total time taken: {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "# Fetch, preprocess, and save data for each mailing list\n",
        "mailing_lists = {\n",
        "\n",
        "    'email-sig': 'https://mail.python.org/pipermail/email-sig/'\n",
        "}\n",
        "\n",
        "for list_name, base_url in mailing_lists.items():\n",
        "    raw_output_file = f'/content/Dissertation_project/{list_name}_emails_raw.txt'\n",
        "    preprocessed_output_file = f'/content/Dissertation_project/{list_name}_emails_preprocessed.txt'\n",
        "\n",
        "    # Fetch and save the mailing list data\n",
        "    fetch_and_save_mailing_list(base_url, raw_output_file)\n",
        "\n",
        "    # Preprocess the raw data\n",
        "    preprocess_text_file_in_chunks(raw_output_file, preprocessed_output_file)\n"
      ],
      "metadata": {
        "id": "lezMoshhu-dr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a3683c2-e6b2-4050-9ab1-eafbeafd074d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing https://mail.python.org/pipermail/email-sig/2015-January.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2015-February.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2015-March.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2015-April.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2015-May.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2015-June.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2015-July.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2015-August.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2015-September.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2015-October.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2015-November.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2015-December.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2016-January.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2016-February.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2016-March.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2016-April.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2016-May.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2016-June.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2016-July.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2016-August.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2016-September.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2016-October.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2016-November.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2016-December.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2017-January.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2017-February.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2017-March.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2017-April.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2017-May.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2017-June.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2017-July.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2017-August.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2017-September.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2017-October.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2017-November.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2017-December.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2018-January.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2018-February.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2018-March.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2018-April.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2018-May.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2018-June.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2018-July.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2018-August.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2018-September.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2018-October.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2018-November.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2018-December.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2019-January.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2019-February.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2019-March.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2019-April.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2019-May.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2019-June.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2019-July.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2019-August.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2019-September.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2019-October.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2019-November.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2019-December.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2020-January.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2020-February.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2020-March.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2020-April.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2020-May.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2020-June.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2020-July.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2020-August.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2020-September.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2020-October.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2020-November.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2020-December.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2021-January.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2021-February.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2021-March.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2021-April.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2021-May.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2021-June.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2021-July.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2021-August.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2021-September.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2021-October.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2021-November.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2021-December.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2022-January.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2022-February.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2022-March.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2022-April.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2022-May.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2022-June.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2022-July.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2022-August.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2022-September.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2022-October.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2022-November.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2022-December.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2023-January.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2023-February.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2023-March.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2023-April.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2023-May.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2023-June.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2023-July.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2023-August.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2023-September.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2023-October.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2023-November.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2023-December.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2024-January.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2024-February.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2024-March.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2024-April.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2024-May.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2024-June.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2024-July.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2024-August.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2024-September.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2024-October.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2024-November.txt\n",
            "Processing https://mail.python.org/pipermail/email-sig/2024-December.txt\n",
            "Data saved to /content/Dissertation_project/email-sig_emails_raw.txt\n",
            "Preprocessed text saved to '/content/Dissertation_project/email-sig_emails_preprocessed.txt'.\n",
            "Total time taken: 0.03 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "APPLICATION OF LDA"
      ],
      "metadata": {
        "id": "VjXwy7kNOZ0L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
        "\n",
        "# Load the preprocessed data\n",
        "file_path = '/content/Dissertation_project/email-sig_emails_preprocessed.txt'\n",
        "with open(file_path, 'r', encoding='utf-8') as f:\n",
        "    preprocessed_emails = f.readlines()\n",
        "\n",
        "# Convert to DataFrame for further processing\n",
        "df = pd.DataFrame({'Cleaned_Content': preprocessed_emails})\n",
        "\n",
        "# Check for empty strings after preprocessing\n",
        "df = df[df['Cleaned_Content'].str.strip() != '']\n",
        "\n",
        "# Vectorize the text data with more topics for finer granularity\n",
        "vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
        "dtm = vectorizer.fit_transform(df['Cleaned_Content'])\n",
        "\n",
        "# Fit LDA model with more topics\n",
        "lda = LDA(n_components=10, random_state=42)\n",
        "lda.fit(dtm)\n",
        "\n",
        "# Display the top words in each topic\n",
        "words = vectorizer.get_feature_names_out()\n",
        "topic_words = lda.components_\n",
        "\n",
        "for i, topic in enumerate(topic_words):\n",
        "    print(f\"Top words in topic #{i}:\")\n",
        "    print(\" \".join([words[j] for j in topic.argsort()[-10:]]))\n",
        "\n",
        "# Assign topics to documents\n",
        "doc_topic_dist = lda.transform(dtm)\n",
        "df['Dominant_Topic'] = doc_topic_dist.argmax(axis=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGdJLmsE05-N",
        "outputId": "54e7981e-839e-410c-ed5e-932c0c06f6f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top words in topic #0:\n",
            "different exception thing issue worth simply want needs point date\n",
            "Top words in topic #1:\n",
            "different exception thing issue worth simply want needs point date\n",
            "Top words in topic #2:\n",
            "object comments simple raised work uses like msg message line\n",
            "Top words in topic #3:\n",
            "different exception thing issue worth simply want needs point date\n",
            "Top words in topic #4:\n",
            "different exception thing issue worth simply want needs point date\n",
            "Top words in topic #5:\n",
            "different exception thing issue worth simply want needs point date\n",
            "Top words in topic #6:\n",
            "different exception thing issue worth simply want needs point date\n",
            "Top words in topic #7:\n",
            "different exception thing issue worth simply want needs point date\n",
            "Top words in topic #8:\n",
            "great creating actually docs libraries read barry post think module\n",
            "Top words in topic #9:\n",
            "encoding right use working way module wed header using issues\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SUBTOPIC MAPPING"
      ],
      "metadata": {
        "id": "kgh90MFhOc6b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example subtopics based on the topics discovered\n",
        "subtopics = {\n",
        "    0: 'attachments',\n",
        "    1: 'compat32',\n",
        "    2: 'email',\n",
        "    3: 'python',\n",
        "    4: 'feedparser',\n",
        "    5: 'headers',\n",
        "    6: 'memory',\n",
        "    7: 'parsing',\n",
        "    8: 'policy'\n",
        "    # These subtopics align with your important keywords\n",
        "}\n",
        "\n",
        "# Map the subtopics to the DataFrame\n",
        "df['Subtopic'] = df['Dominant_Topic'].map(subtopics)\n",
        "\n",
        "# Verify that the 'Subtopic' column exists\n",
        "if 'Subtopic' in df.columns:\n",
        "    print(\"Subtopic column created successfully.\")\n",
        "else:\n",
        "    print(\"Failed to create Subtopic column.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wM6Hr3HB06A0",
        "outputId": "f6d22c92-cd77-46e3-aed5-af2e82266d70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subtopic column created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SENTIMENT ANALYSIS USING VADER , BERT , DISTILBERT"
      ],
      "metadata": {
        "id": "KtklwJYrOgKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from transformers import pipeline\n",
        "import nltk\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Initialize sentiment analysis tools\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "bert_classifier = pipeline('sentiment-analysis', model='nlptown/bert-base-multilingual-uncased-sentiment', max_length=512, truncation=True)\n",
        "distilbert_classifier = pipeline('sentiment-analysis', model='distilbert-base-uncased-finetuned-sst-2-english', max_length=512, truncation=True)\n",
        "\n",
        "# Function to calculate sentiment for a given text\n",
        "def calculate_sentiments(text):\n",
        "    vader_score = sia.polarity_scores(text)['compound']\n",
        "    bert_label = bert_classifier(text)[0]['label']\n",
        "    distilbert_label = distilbert_classifier(text)[0]['label']\n",
        "    return vader_score, bert_label, distilbert_label\n",
        "\n",
        "# Apply sentiment analysis to the DataFrame\n",
        "df['VADER Sentiment'], df['BERT Sentiment'], df['DistilBERT Sentiment'] = zip(*df['Cleaned_Content'].apply(calculate_sentiments))\n",
        "\n",
        "# Adjust sentiment scoring\n",
        "def adjust_sentiment(vader_score, bert_label, distilbert_label):\n",
        "    bert_score = {'1 star': -1, '2 stars': -0.5, '3 stars': 0, '4 stars': 0.5, '5 stars': 1}.get(bert_label, 0)\n",
        "    distilbert_score = {'NEGATIVE': -1, 'POSITIVE': 1, 'NEUTRAL': 0}.get(distilbert_label, 0)\n",
        "\n",
        "    # Increase weight for VADER due to its consistent positive output\n",
        "    combined_score = (vader_score * 0.6 + bert_score * 0.2 + distilbert_score * 0.2) / 1.0\n",
        "\n",
        "    if combined_score > 0.2:\n",
        "        return 'POSITIVE'\n",
        "    elif combined_score < -0.2:\n",
        "        return 'NEGATIVE'\n",
        "    else:\n",
        "        return 'NEUTRAL'\n",
        "\n",
        "# Apply the adjusted sentiment analysis\n",
        "df['Adjusted Sentiment'] = df.apply(lambda row: adjust_sentiment(row['VADER Sentiment'], row['BERT Sentiment'], row['DistilBERT Sentiment']), axis=1)\n",
        "\n",
        "# Group by subtopic to analyze the sentiment distribution\n",
        "sentiment_analysis = df.groupby('Subtopic').agg({\n",
        "    'VADER Sentiment': lambda x: x.mode()[0],\n",
        "    'BERT Sentiment': lambda x: x.mode()[0],\n",
        "    'DistilBERT Sentiment': lambda x: x.mode()[0],\n",
        "    'Adjusted Sentiment': lambda x: x.mode()[0]\n",
        "})\n",
        "\n",
        "# Save to a text file\n",
        "output_file = '/content/sentiment_analysis_results.txt'\n",
        "with open(output_file, 'w') as f:\n",
        "    f.write(\"Sentiment analysis across subtopics:\\n\")\n",
        "    f.write(\"====================================\\n\")\n",
        "    for subtopic, row in sentiment_analysis.iterrows():\n",
        "        f.write(f\"Subtopic: {subtopic}\\n\")\n",
        "        f.write(f\"  VADER Sentiment: {row['VADER Sentiment']}\\n\")\n",
        "        f.write(f\"  BERT Sentiment: {row['BERT Sentiment']}\\n\")\n",
        "        f.write(f\"  DistilBERT Sentiment: {row['DistilBERT Sentiment']}\\n\")\n",
        "        f.write(f\"  Adjusted Sentiment: {row['Adjusted Sentiment']}\\n\")\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "print(f\"Results saved to {output_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4X81g1a06C_",
        "outputId": "59c6632b-e0ef-42ed-cb36-97040f957a9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to /content/sentiment_analysis_results.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DOWNLOAD THE RESULT FILE"
      ],
      "metadata": {
        "id": "aGTxkB98Ok1F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Specify the path to the CSV file\n",
        "file_path = '/content/sentiment_analysis_results.txt'\n",
        "\n",
        "# Download the file\n",
        "files.download(file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "fnZnccEbl-un",
        "outputId": "bd68e7fa-5545-4e05-b819-6acd6f9e5a0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_002ce2b8-220d-4aa4-a91f-5732073878bd\", \"sentiment_analysis_results.txt\", 337)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}